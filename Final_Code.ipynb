{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.transforms import ToTensor\n",
    "from torchvision import models\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "real_dir = './Input/real_and_fake_face_detection/real_and_fake_face/training_real/'\n",
    "real_path = os.listdir(real_dir)\n",
    "\n",
    "fake_dir = './Input/real_and_fake_face_detection/real_and_fake_face/training_fake/'\n",
    "fake_path = os.listdir(fake_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_img(path):\n",
    "    '''Loading images from directory and changing color space from cv2 standard BGR to RGB for better visualization'''\n",
    "    image = cv2.imread(path)\n",
    "    image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    return image_rgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(20, 20))\n",
    "\n",
    "for i in range(50):\n",
    "    plt.subplot(10, 10, i+1)\n",
    "    plt.imshow(load_img(real_dir + real_path[i]))\n",
    "    plt.suptitle(\"Real faces\", fontsize=20)\n",
    "    plt.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(20, 20))\n",
    "\n",
    "for i in range(50):\n",
    "    plt.subplot(10, 10, i+1)\n",
    "    plt.imshow(load_img(fake_dir + fake_path[i]))\n",
    "    plt.suptitle(\"Fake faces\", fontsize=20)\n",
    "    plt.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, fake faces have some weird elements on them, like inaccurately inserted fragments, spots, illuminated areas. The main task for CNNs is to find this weird features.Now I collect all images to DataFrame for ease dataset formation\n",
    "\n",
    "\n",
    "Collecting all images to DataFrame for ease dataset formation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "real_df = pd.DataFrame({'image_path': real_dir + real_path[i], 'label': 1} for i in range(0, 1081))\n",
    "fake_df = pd.DataFrame({'image_path': fake_dir + fake_path[i], 'label': 0} for i in range(0, 960))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([real_df, fake_df], ignore_index=True)\n",
    "df.tail(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = shuffle(df)\n",
    "df = df.reset_index(drop=True)\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, val_df = train_test_split(df, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding some configuration here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_size = 224\n",
    "batch_size = 64\n",
    "num_epochs = 10\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining transforms for train and validation images, visual transforms is for visualization function that shows one batch. All transforms, except of usual ones like ToTensor, have been chosen because they can help CNNs to find weird features well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_transforms = {'train_transform': A.Compose([A.Resize(image_size, image_size), \n",
    "                                                  A.HorizontalFlip(p=0.5), \n",
    "                                                  A.RandomBrightnessContrast(always_apply=False, \n",
    "                                                                             p=0.4),\n",
    "                                                  A.Solarize(always_apply=False, \n",
    "                                                             p=0.4, \n",
    "                                                             threshold=(42, 42)),\n",
    "                                                  A.MultiplicativeNoise(always_apply=False, \n",
    "                                                                        p=0.8, \n",
    "                                                                        multiplier=(0.6800000071525574, 1.409999966621399), \n",
    "                                                                        per_channel=True, \n",
    "                                                                        elementwise=True),\n",
    "                                                  A.Normalize(mean=(0.485, 0.456, 0.406), \n",
    "                                                              std=(0.229, 0.224, 0.225), \n",
    "                                                              max_pixel_value=255.0, \n",
    "                                                              p=1.0), \n",
    "                                                  ToTensorV2()]),\n",
    "                    \n",
    "                   'validation_transform': A.Compose([A.Resize(image_size, image_size), \n",
    "                                                      A.Normalize(mean=(0.485, 0.456, 0.406), \n",
    "                                                                  std=(0.229, 0.224, 0.225), \n",
    "                                                                  max_pixel_value=255.0, \n",
    "                                                                  p=1.0), \n",
    "                                                      ToTensorV2()]),\n",
    "                   'visualization_transform': A.Compose([A.Resize(image_size, image_size), \n",
    "                                                         A.HorizontalFlip(p=0.5), \n",
    "                                                         A.RandomBrightnessContrast(always_apply=False, \n",
    "                                                                                    p=0.4),\n",
    "                                                  A.Solarize(always_apply=False, \n",
    "                                                             p=0.4, \n",
    "                                                             threshold=(42, 42)),\n",
    "                                                  A.MultiplicativeNoise(always_apply=False, \n",
    "                                                                        p=0.8, \n",
    "                                                                        multiplier=(0.6800000071525574, 1.409999966621399), \n",
    "                                                                        per_channel=True, \n",
    "                                                                        elementwise=True)])}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining Dataset class, there is no magic here, just what PyTorch tutorial learn us:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, image_labels, image_dir, transform=None, target_transform=None):\n",
    "        self.image_labels = image_labels\n",
    "        self.image_dir = image_dir\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "        \n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.image_labels)\n",
    "    \n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        image_path = self.image_dir.iloc[index]\n",
    "        image = cv2.imread(image_path)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        label = self.image_labels.iloc[index]\n",
    "        if self.transform:\n",
    "            image = self.transform(image=image)['image']\n",
    "        if self.target_transform:\n",
    "            label = self.target_transform(label=label)\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_label = train_df['label']\n",
    "train_features = train_df['image_path']\n",
    "\n",
    "val_label = val_df['label']\n",
    "val_features = val_df['image_path']\n",
    "\n",
    "train_dataset = ImageDataset(train_label, \n",
    "                             train_features, \n",
    "                             transform=image_transforms['train_transform'])\n",
    "val_dataset = ImageDataset(val_label, \n",
    "                           val_features, \n",
    "                           transform=image_transforms['validation_transform'])\n",
    "visual_train_dataset =  ImageDataset(train_label, \n",
    "                                     train_features, \n",
    "                                     transform=image_transforms['visualization_transform'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And of course DataLoaders:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True)\n",
    "visual_loader = DataLoader(visual_train_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking if there is all OK in DataLoaders:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visual_train_f, visual_train_t = next(iter(visual_loader))\n",
    "print(f'Feature batch shape: {visual_train_f.size()}')\n",
    "print(f'Target batch shape: {visual_train_t.size()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in visual_loader:\n",
    "    img, label = item[0], item[1]\n",
    "    print(img, label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seems like everything is OK. Now let's see how one batch of augmented images looks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_batch(features, target, batch_size=batch_size):\n",
    "    '''Shows one batch of augmented images'''\n",
    "    plt.figure(figsize=(10, 40))\n",
    "    for i in range(batch_size):\n",
    "        img = features[i]\n",
    "        label = target[i]\n",
    "        \n",
    "        plt.subplot(16, 4, i+1)\n",
    "        plt.title(f'Class: {label}')\n",
    "        plt.imshow(img)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_batch(visual_train_f, visual_train_t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's time to define and train some CNNs. First of them will be a little custom CNN FaceNet, I define it's architecture by myself and train it with a simple training loop:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FaceNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(FaceNet, self).__init__()\n",
    "        self.conv_1 = nn.Conv2d(in_channels=3, out_channels=18, kernel_size=3)\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=2)\n",
    "        self.batchnorm_1 = nn.BatchNorm2d(18)\n",
    "        self.conv_2 = nn.Conv2d(in_channels=18, out_channels=18, kernel_size=3)\n",
    "        self.batchnorm_2 = nn.BatchNorm2d(18)\n",
    "        self.conv_3 = nn.Conv2d(in_channels=18, out_channels=32, kernel_size=3)\n",
    "        self.fc_1 = nn.Linear(21632, 128)\n",
    "        self.fc_2 = nn.Linear(128, 64)\n",
    "        self.classifier = nn.Linear(64, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.maxpool(nn.functional.relu(self.conv_1(x)))\n",
    "        x = self.maxpool(nn.functional.relu(self.conv_2(x)))\n",
    "        x = self.maxpool(nn.functional.relu(self.conv_3(x)))\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = nn.functional.relu(self.fc_1(x))\n",
    "        x = nn.functional.relu(self.fc_2(x))\n",
    "        x = torch.sigmoid(self.classifier(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_custom = FaceNet()\n",
    "model_custom.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion_custom = torch.nn.BCELoss()\n",
    "optimizer_custom = torch.optim.Adam(model_custom.parameters(), lr=0.0001, weight_decay=1e-5)\n",
    "scheduler_custom = torch.optim.lr_scheduler.ExponentialLR(optimizer_custom, gamma=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_loop(model, training_loader, validation_loader, criterion, optimizer, scheduler, epochs=num_epochs):\n",
    "    '''Training loop for train and eval modes'''\n",
    "    for epoch in range(1, epochs+1):\n",
    "        model.train()\n",
    "        train_accuracy = 0\n",
    "        train_loss = 0\n",
    "        for image, target in training_loader:\n",
    "            image = image.to(device)\n",
    "            target = target.to(device)\n",
    "            target = target.unsqueeze(1)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = torch.sigmoid(model(image))\n",
    "            loss = criterion(outputs.float(), target.float())\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "            train_accuracy += ((outputs > 0.5) == target).float().mean().item()\n",
    "            \n",
    "        with torch.no_grad():\n",
    "            model.eval()\n",
    "            valid_loss = 0\n",
    "            val_accuracy = 0\n",
    "            for val_image, val_target in validation_loader:\n",
    "                val_image = val_image.to(device)\n",
    "                val_target = val_target.to(device)\n",
    "                val_target = val_target.unsqueeze(1)\n",
    "                val_outputs = torch.sigmoid(model(val_image))\n",
    "                val_loss = criterion(val_outputs.float(), val_target.float())\n",
    "                \n",
    "                valid_loss += val_loss.item()\n",
    "                val_accuracy += ((val_outputs > 0.5) == val_target).float().mean().item() \n",
    "                \n",
    "        print(f'Epoch: {epoch} Train loss: {train_loss/len(training_loader)} Train accuracy: {train_accuracy /len(training_loader)} Val loss: {valid_loss/len(validation_loader)} Val accuracy: {val_accuracy/len(validation_loader)}')\n",
    "        scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_loop(model_custom, \n",
    "              train_loader, \n",
    "              val_loader, \n",
    "              criterion_custom, \n",
    "              optimizer_custom, \n",
    "              scheduler_custom, \n",
    "              epochs=num_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Second model is tre ResNet-50, fine stable architecture, not pretrained one, I'll take it from torchvision.models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_np = models.resnet50(pretrained=False)\n",
    "model_np.fc = nn.Sequential(nn.Linear(in_features=2048, out_features=512, bias=True), \n",
    "                     nn.ReLU(inplace=True),\n",
    "                     nn.Linear(in_features=512, out_features=1, bias=True))\n",
    "\n",
    "model_np.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion_np = torch.nn.BCELoss()\n",
    "optimizer_np = torch.optim.Adam(model_np.parameters(), lr=0.00001, weight_decay=1e-5)\n",
    "scheduler_np = torch.optim.lr_scheduler.ExponentialLR(optimizer_np, gamma=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_loop(model_np, \n",
    "              train_loader, \n",
    "              val_loader, \n",
    "              criterion_np, \n",
    "              optimizer_np, \n",
    "              scheduler_np, \n",
    "              epochs=num_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the last is ResNet-50 pretrained on Imagenet dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_p = models.resnet50(pretrained=True)\n",
    "model_p.fc = nn.Sequential(nn.Linear(in_features=2048, out_features=512, bias=True), \n",
    "                     nn.ReLU(inplace=True),\n",
    "                     nn.Linear(in_features=512, out_features=1, bias=True))\n",
    "for param in model_p.parameters():\n",
    "    param.requires_grad = True\n",
    "model_p.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion_p = torch.nn.BCELoss()\n",
    "optimizer_p = torch.optim.Adam(model_p.parameters(), lr=0.00001, weight_decay=1e-5)\n",
    "scheduler_p = torch.optim.lr_scheduler.ExponentialLR(optimizer_p, gamma=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_loop(model_p, \n",
    "              train_loader, \n",
    "              val_loader, \n",
    "              criterion_p, \n",
    "              optimizer_p, \n",
    "              scheduler_p, \n",
    "              epochs=num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchcam.methods import GradCAMpp\n",
    "from torchcam.utils import overlay_mask\n",
    "from torchvision.io.image import read_image\n",
    "from torchvision.transforms.functional import normalize, resize, to_pil_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_path = './Input/real-and-fake-face-detection/training_fake/easy_125_0011.jpg'\n",
    "img = read_image(img_path)\n",
    "fig = plt.figure(figsize=(8, 8))\n",
    "plt.imshow(to_pil_image(img))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
